{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 20 days\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float32, vector)>)]\n",
      "Looping 1000 times took 2.341017 seconds\n",
      "Result is [ 1.23178029  1.61879337  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323284]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in xrange(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        #print 'u', shape\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named fuel.datasets.mnist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a02f3e0979ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mScaleAndShift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreams\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataStream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschemes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequentialScheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mShuffledScheme\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named fuel.datasets.mnist"
     ]
    }
   ],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "\n",
    "                         \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset=slice(None,40000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(\n",
    "    cifar10_train,\n",
    "    iteration_scheme=ShuffledScheme(cifar10_train.num_examples, 25))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset=slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(\n",
    "    cifar10_validation, iteration_scheme=SequentialScheme(cifar10_validation.num_examples, 100))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(\n",
    "    cifar10_test, iteration_scheme=SequentialScheme(cifar10_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (100, 1, 28, 28) containing float32\n",
      " - an array of size (100, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (250, 1, 28, 28) containing float32\n",
      " - an array of size (250, 1) containing uint8\n",
      "CIFAR: \n",
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"CIFAR: \"  \n",
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-6604dab46423>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-6604dab46423>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    iris_train_f = iris[:2*pop_num/3,1:]\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#print iris\n",
    "\n",
    "feats = 4\n",
    "alpha = 0.01\n",
    "pop_num = 150\n",
    "rng = np.random\n",
    "iris_f = iris['data'][:pop_num,:feats]\n",
    "iris_t = iris['target'][:pop_num]\n",
    "iris = hstack(([[x] for x in iris_t], iris_f))\n",
    "\n",
    "rng.shuffle(iris\n",
    "\n",
    "#print iris\n",
    "\n",
    "iris_train_f = iris[:2*pop_num/3,1:]\n",
    "iris_train_t = np.array(iris[:2*pop_num/3, 0], dtype='uint8')\n",
    "iris_test_f = iris[2*pop_num/3:,1:]\n",
    "iris_test_t = np.array(iris[2*pop_num/3:, 0], dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.printing as TP\n",
    "from IPython.display import SVG\n",
    "def svgdotprint(g):\n",
    "    return SVG(theano.printing.pydotprint(g, return_image=True, format='svg'))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor.signal.downsample as down\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, lamb = 0.1,rng=None, name=\"\"):\n",
    "        self.name = name\n",
    "        self.lamb = lamb\n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState(1234)\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return []\n",
    "    \n",
    "    def update(self, foo, alpha):\n",
    "        return []\n",
    "    def cost(self, gamma):\n",
    "        return 0;\n",
    "    def setInputDim(self, inputDim):\n",
    "        self.num_out = inputDim\n",
    "    def getOutputDim(self):\n",
    "        return self.num_out\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    @property\n",
    "    def moments(self):\n",
    "        return []\n",
    "    def setLambda(self, lamb):\n",
    "        self.lamb = lamb\n",
    "    \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_out, initW = 10., gamma  = 0.1, n = \"\", weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(name= n, **kwargs)\n",
    "        self.num_out = num_out\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(initW / (num_out))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        self.gamma= theano.shared(gamma)\n",
    "        self.b = theano.shared(Constant(0.0).generate(self.rng, (num_out)), name=self.name +\" bias\")\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    @property\n",
    "    def moments(self):\n",
    "        return [self.mW, self.mb]\n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def build(self, X):\n",
    "        #print self.name+ \" \",X.shape \n",
    "        return X.dot(self.W) + self.b\n",
    "    def cost(self, gamma):\n",
    "        return  (self.W ** 2).sum() * gamma\n",
    "    def update(self, foo, alpha):\n",
    "        gw, gb = T.grad(foo, self.parameters)\n",
    "        moments = self.lamb * self.moments -  [alpha *gw, alpha *gb]\n",
    "        self.setMoments(moments)\n",
    "        return  [(self.W, self.W + moments[0]), \n",
    "                 (self.b, self.b + moments[1])]\n",
    "    def setInputDim(self, inputDim):\n",
    "        shape = (inputDim, self.num_out)\n",
    "        print \"AffineLayer: \", shape\n",
    "        print \"AffineLayerVel : \", (shape, self.b.shape)\n",
    "        self.W = theano.shared(IsotropicGaussian(0.01).generate(self.rng, shape),name=self.name +\" weight\")\n",
    "        self.mW = theano.shared(zeros_like(self.W, dtype='float32'))\n",
    "        self.mb = theano.shared(zeros_like(self.W, dtype='float32'))\n",
    "    \n",
    "class LogRegLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(LogRegLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.nnet.sigmoid(X)\n",
    "\n",
    "\n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(TanhLayer, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        print \"tanh layer\", X\n",
    "        return T.tanh(X)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(ReLULayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.maximum(0.0, X)\n",
    "\n",
    "class Conv(Layer):\n",
    "    def __init__(self, f_out, f_size, initW = 10., lamb = 0.9, n = \"\", weight_init = None, **kwargs):\n",
    "        super(Conv, self).__init__(name = n, **kwargs)\n",
    "        if weight_init is None:\n",
    "            b = numpy.sqrt(initW / (f_out+ f_size + f_size))\n",
    "            self.weight_init = Uniform(width=b)\n",
    "        self.f_out = f_out\n",
    "        self.f_size = f_size\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.F, self.b]\n",
    "    @property\n",
    "    def moments(self):\n",
    "        return [self.mF, self.mb]\n",
    "    \n",
    "    def setInputDim(self, inputDim):\n",
    "        F_size = (self.f_out, ) + (inputDim[0], self.f_size, self.f_size)                                   \n",
    "        self.num_out = (self.f_out, inputDim[1] - self.f_size + 1, inputDim[2] - self.f_size + 1)\n",
    "        print 'Conv filter', F_size\n",
    "        self.F = theano.shared(IsotropicGaussian(0.01).generate(self.rng, F_size),name=self.name +\" filter\")\n",
    "        self.b = theano.shared(Constant(0.0).generate(self.rng, (self.f_out, )), name='CB')\n",
    "        self.mF = theano.shared(zeros_like(self.F, dtype='float32'), name='ConvM')\n",
    "        self.mb = theano.shared(zeros_like(self.b, dtype='float32'), name='Convb')\n",
    "    \n",
    "    \n",
    "    def update(self, foo, alpha):\n",
    "        gf, gb = theano.grad(foo, [self.F, self.b])\n",
    "        updates = []\n",
    "        self.mF= self.lamb*self.mF  -  gf\n",
    "        self.mb = self.lamb*self.mb - gb\n",
    "        F_new = self.F + self.mF\n",
    "        b_new = self.b + self.mb \n",
    "        return [(self.F, F_new), (self.b, b_new)]\n",
    "     \n",
    "    \n",
    "    def build(self, X):\n",
    "        conv = theano.tensor.nnet.conv2d(X, self.F) + self.b.dimshuffle('x',0,'x','x')\n",
    "        return theano.tensor.maximum(0.0, conv)\n",
    "        \n",
    "        \n",
    "class Flatten(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(Flatten, self).__init__(name = n, **kwargs)\n",
    "    def build(self, X):\n",
    "        return T.flatten(X, 2)\n",
    "    def setInputDim(self, inputDim):\n",
    "        out_dim = 1\n",
    "        for i in inputDim:\n",
    "            out_dim = out_dim * i\n",
    "        self.num_out = out_dim\n",
    "    \n",
    "\n",
    "class BNLayer(Layer):\n",
    "    def __init__(self,num_out, n = \"BNLayer\", gamma = 0.1, alpha=1.0,**kwargs):\n",
    "        super(BNLayer, self).__init__(name = n, **kwargs)\n",
    "        self.num_out, self.alpha = num_out, alpha\n",
    "        self.gamma= theano.shared(gamma)\n",
    "    def build(self, X):\n",
    "        self.Gamma = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Gamma \" + self.name))\n",
    "        print 'Gamma shape:', np.zeros((1, self.num_out)).shape\n",
    "        self.Beta  = theano.shared(np.zeros((self.num_out,), dtype='float32'), name=(\"Beta \" + self.name))\n",
    "        print 'Beta shape:', np.zeros((1, self.num_out)).shape\n",
    "        self.Gamma.tag.initializer = Constant(1.0)\n",
    "        self.Beta.tag.initializer = Constant(0.0)\n",
    "    \n",
    "        self.means = self.alpha * theano.tensor.mean(X, 0, keepdims=True)\n",
    "        self.stds = self.alpha * theano.tensor.std(X, 0, keepdims=True)\n",
    "        self.means.tag.initializer = Constant(0.0)\n",
    "        self.stds.tag.initializer = Constant(1.0)\n",
    "        \n",
    "        normalized = theano.tensor.nnet.bn.batch_normalization(\n",
    "            X,\n",
    "            self.Gamma,\n",
    "            self.Beta,\n",
    "            self.means,\n",
    "            self.stds,\n",
    "            'high_mem'\n",
    "        )\n",
    "        return normalized\n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.Gamma, self.Beta]\n",
    "    @property\n",
    "    def check(self):\n",
    "        return [self.gg, self.gb, self.Gamma, self.Beta,self.means, self.stds ]\n",
    "    #def cost(self):\n",
    "    #    return  ((self.Gamma ** 2).sum() + (self.Gamma ** 2).sum())* self.gamma\n",
    "    def update(self, foo, alpha):\n",
    "        self.gg, self.gb = T.grad(foo, self.parameters)\n",
    "        return  [(self.Gamma, self.Gamma- alpha *self.gg),\n",
    "            (self.Beta, self.Beta - alpha * self.gb)] \n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, n = \"\", **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(name = n, **kwargs)\n",
    "    \n",
    "    def build(self, X):\n",
    "        return T.nnet.softmax(X)\n",
    "\n",
    "class MaxPoolLayer(Layer):\n",
    "    def __init__(self, p_size, n = \"MP\", **kwargs):\n",
    "        super(MaxPoolLayer, self).__init__(name = n, **kwargs)\n",
    "        self.p_size = p_size\n",
    "    def build(self, input):\n",
    "        return down.max_pool_2d(input, (self.p_size,self.p_size), ignore_border=True)\n",
    "    def getOutputDim(self):\n",
    "        shape = (self.num_out[0], ) + (self.num_out[1]/self.p_size, self.num_out[2]/self.p_size) \n",
    "        print \"maxPool\", shape\n",
    "        return shape\n",
    "    \n",
    "class DropOutLayer(Layer):\n",
    "    def __init__(self, dropOut = 0.1, n = \"MP\", **kwargs):\n",
    "        super(DropOutLayer, self).__init__(name = n, **kwargs)\n",
    "        self.dropOut = dropOut\n",
    "        self.u = Uniform(0.5, 1.)\n",
    "    def build(self, input):\n",
    "        self.D = theano.shared((self.u.generate(self.rng, (self.num_out,))>= self.dropOut) + 0,name=self.name +\" Dropout\") \n",
    "        print self.D.get_value()\n",
    "        return input * self.D\n",
    "    def getOutputDim(self):\n",
    "        shape = self.num_out \n",
    "        print \"maxPool\", shape\n",
    "        return shape\n",
    "    def update(self, foo, alpha):\n",
    "        return  [(self.D,\n",
    "            (self.u.generate(self.rng, (self.num_out,)) >= self.dropOut)+0)]\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None, lamb = 0.1):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "        self.lamb = lamb\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def build(self, inputDim):\n",
    "        x = T.tensor4(\"x\")\n",
    "        y = T.vector(\"y\", dtype='int64')\n",
    "        cost = 0\n",
    "        moments = []\n",
    "        params = []\n",
    "        updates = []\n",
    "        X = x\n",
    "\n",
    "    \n",
    "        \n",
    "        alpha = theano.tensor.scalar('alpha',dtype='float32')\n",
    "        lamb = theano.tensor.scalar('alpha',dtype='float32')\n",
    "        gamma = theano.tensor.scalar('alpha',dtype='float32')\n",
    "        \n",
    "        \n",
    "        for layer, i in zip(self.layers, range(len(self.layers))):\n",
    "            layer.setInputDim(inputDim)\n",
    "            moments += layer.moments\n",
    "            params += layer.parameters\n",
    "            inputDim = layer.getOutputDim()\n",
    "            X = layer.build(X)\n",
    "            cost += layer.cost(gamma)\n",
    "            \n",
    "        pred = np.argmax(X, 1)\n",
    "        error_rate = theano.tensor.neq(pred, y.ravel()).mean()\n",
    "        nll = -theano.tensor.log(X[theano.tensor.arange(y.shape[0]), y.ravel()]).mean() \n",
    "\n",
    "        self.costFoo = nll + cost\n",
    "            \n",
    "        grads = theano.grad(self.costFoo, params)\n",
    "        # for some reasons i have to get moments this way. \n",
    "        moments = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in params]\n",
    "\n",
    "        for p,g,v in zip(params, grads, moments):\n",
    "            print g\n",
    "            print v\n",
    "            v_new = lamb*v - alpha*g\n",
    "            print v_new\n",
    "            p_new = p + v_new\n",
    "            updates += [(v,v_new), (p,p_new)]\n",
    "\n",
    "        self.train = theano.function(inputs=[x,y, alpha, lamb, gamma], \n",
    "                                    outputs=[pred, self.costFoo, alpha],\n",
    "                                    updates=updates)\n",
    "        self.predict  = theano.function(inputs=[x], \n",
    "                                    outputs=pred)\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def trainFunction(self):\n",
    "        return self.train\n",
    "    \n",
    "    @property\n",
    "    def predictFunction(self):\n",
    "        return self.predict\n",
    "    @property\n",
    "    def costFunction(self):\n",
    "        return self.costFoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_er(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        predictions = net.predictFunction(X)\n",
    "        #print predictions != Y.ravel()\n",
    "        num_errs += (predictions != Y.ravel()).sum()\n",
    "        #print Y.shape[0], num_errs\n",
    "        num_examples += Y.shape[0]\n",
    "    return num_errs/num_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Conv filter (50, 3, 3, 3)\n",
      "maxPool (50, 10, 10)\n",
      "Conv filter (50, 50, 3, 3)\n",
      "maxPool (50, 4, 4)\n",
      "AffineLayer:  (800, 1000)\n",
      "AffineLayerVel :  ((800, 1000), Shape.0)\n",
      "maxPool 1000\n",
      "[0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0\n",
      " 1 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1 1 1 0\n",
      " 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1\n",
      " 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0\n",
      " 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1\n",
      " 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
      " 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1 0 0\n",
      " 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0\n",
      " 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1\n",
      " 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 1\n",
      " 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
      " 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1\n",
      " 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 0\n",
      " 1 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1\n",
      " 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1\n",
      " 1]\n",
      "AffineLayer:  (1000, 10)\n",
      "AffineLayerVel :  ((1000, 10), Shape.0)\n",
      "AbstractConv2d_gradWeights{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=None, kshp=None}.0\n",
      "V_ filter\n",
      "Elemwise{sub,no_inplace}.0\n",
      "GpuFromHost.0\n",
      "V_CB\n",
      "Elemwise{sub,no_inplace}.0\n",
      "AbstractConv2d_gradWeights{border_mode='valid', subsample=(1, 1), filter_flip=True, imshp=None, kshp=None}.0\n",
      "V_ filter\n",
      "Elemwise{sub,no_inplace}.0\n",
      "GpuFromHost.0\n",
      "V_CB\n",
      "Elemwise{sub,no_inplace}.0\n",
      "Elemwise{add,no_inplace}.0\n",
      "V_tA weight\n",
      "Elemwise{sub,no_inplace}.0\n",
      "GpuFromHost.0\n",
      "V_tA bias\n",
      "Elemwise{sub,no_inplace}.0\n",
      "Elemwise{add,no_inplace}.0\n",
      "V_tA weight\n",
      "Elemwise{sub,no_inplace}.0\n",
      "GpuFromHost.0\n",
      "V_tA bias\n",
      "Elemwise{sub,no_inplace}.0\n",
      "Start\n",
      "gamma:  0.0005\n",
      "0.00999999977648 2.33832179489 0.92\n",
      "0.00999999977648 2.34992863841 0.88\n",
      "0.00999999977648 2.34877178187 0.96\n",
      "0.00999999977648 2.33583378871 0.92\n",
      "0.00999999977648 2.32608398903 0.96\n",
      "0.00999999977648 1.8302218912 0.64\n",
      "0.00999999977648 1.8734357589 0.64\n",
      "0.00999999977648 1.77924891127 0.8\n",
      "0.00999999977648 2.21168121919 0.8\n",
      "0.00999999977648 1.85240136096 0.72\n",
      "0.00999999977648 1.79194967906 0.64\n",
      "0.00999999977648 1.83082688113 0.56\n",
      "0.00999999977648 1.53147753888 0.52\n",
      "0.00999999977648 1.6774925937 0.48\n",
      "0.00999999977648 1.40604028812 0.6\n",
      "0.00999999977648 1.40864785341 0.44\n",
      "After epoch:  0 0.5182 time:  15.2450041771\n",
      "0.00999999977648 1.40938529321 0.52\n",
      "0.00999999977648 1.14138108705 0.36\n",
      "0.00999999977648 1.34512212469 0.56\n",
      "0.00999999977648 1.57665752367 0.48\n",
      "0.00999999977648 1.75850330985 0.44\n",
      "0.00999999977648 1.41562420845 0.52\n",
      "0.00999999977648 1.03778360745 0.44\n",
      "0.00999999977648 1.5091882413 0.44\n",
      "0.00999999977648 1.31753460892 0.48\n",
      "0.00999999977648 0.880716750045 0.32\n",
      "0.00999999977648 1.57182157941 0.48\n",
      "0.00999999977648 1.26679516743 0.44\n",
      "0.00999999977648 1.19638059822 0.32\n",
      "0.00999999977648 1.55649864676 0.44\n",
      "0.00999999977648 1.21377678455 0.32\n",
      "0.00999999977648 1.19374870256 0.36\n",
      "After epoch:  1 0.3715 time:  15.2197740078\n",
      "0.00999999977648 1.3146252124 0.48\n",
      "0.00999999977648 1.33428798614 0.52\n",
      "0.00999999977648 1.10558076943 0.4\n",
      "0.00999999977648 1.11262233868 0.44\n",
      "0.00999999977648 1.37361641934 0.36\n",
      "0.00999999977648 1.23971301223 0.4\n",
      "0.00999999977648 0.795779604856 0.24\n",
      "0.00999999977648 0.94103254491 0.28\n",
      "0.00999999977648 1.11721458811 0.32\n",
      "0.00999999977648 1.43887166196 0.44\n",
      "0.00999999977648 0.938429231939 0.32\n",
      "0.00999999977648 1.02020937797 0.36\n",
      "0.00999999977648 1.03577822099 0.28\n",
      "0.00999999977648 1.02865476459 0.28\n",
      "0.00999999977648 1.19046387997 0.4\n",
      "0.00999999977648 0.693122282038 0.2\n",
      "After epoch:  2 0.3455 time:  15.2010340691\n",
      "0.00999999977648 1.5114156565 0.6\n",
      "0.00999999977648 0.884930656042 0.28\n",
      "0.00999999977648 1.07821003486 0.4\n",
      "0.00999999977648 0.805018728662 0.08\n",
      "0.00999999977648 0.778683352606 0.24\n",
      "0.00999999977648 0.985264401704 0.32\n",
      "0.00999999977648 0.687865376694 0.2\n",
      "0.00999999977648 1.60790844812 0.48\n",
      "0.00999999977648 0.762803051657 0.24\n",
      "0.00999999977648 0.737588037367 0.24\n",
      "0.00999999977648 1.10052877646 0.4\n",
      "0.00999999977648 1.0825693704 0.28\n",
      "0.00999999977648 0.940386669498 0.36\n",
      "0.00999999977648 0.880788172694 0.32\n",
      "0.00999999977648 0.659199619178 0.12\n",
      "0.00999999977648 0.561383761284 0.2\n",
      "After epoch:  3 0.296 time:  15.1953599453\n",
      "0.00999999977648 0.981711788303 0.36\n",
      "0.00999999977648 0.641176506425 0.16\n",
      "0.00999999977648 0.57414896282 0.2\n",
      "0.00999999977648 1.05466829712 0.32\n",
      "0.00999999977648 0.838404457303 0.2\n",
      "0.00999999977648 0.856180895689 0.2\n",
      "0.00999999977648 0.869927753271 0.2\n",
      "0.00999999977648 0.812731461397 0.2\n",
      "0.00999999977648 0.761958519041 0.2\n",
      "0.00999999977648 0.884411835615 0.24\n",
      "0.00999999977648 0.576084401369 0.08\n",
      "0.00999999977648 0.663111665181 0.12\n",
      "0.00999999977648 0.980992085106 0.36\n",
      "0.00999999977648 1.13255013067 0.4\n",
      "0.00999999977648 0.8275687775 0.32\n",
      "0.00999999977648 0.726374638221 0.2\n",
      "After epoch:  4 0.2828 time:  15.185008049\n",
      "0.00999999977648 0.418996933043 0.04\n",
      "0.00999999977648 0.807140092052 0.16\n",
      "0.00999999977648 0.378428673905 0.08\n",
      "0.00999999977648 0.679390962505 0.12\n",
      "0.00999999977648 1.03159577771 0.32\n",
      "0.00999999977648 1.4647088052 0.32\n",
      "0.00999999977648 0.507532484734 0.12\n",
      "0.00999999977648 0.810941628115 0.28\n",
      "0.00999999977648 0.719362004264 0.24\n",
      "0.00999999977648 1.3307950291 0.36\n",
      "0.00999999977648 0.719210718162 0.12\n",
      "0.00999999977648 0.751765974801 0.2\n",
      "0.00999999977648 1.00664987907 0.28\n",
      "0.00999999977648 1.23921052982 0.32\n",
      "0.00999999977648 0.884425658495 0.2\n",
      "0.00999999977648 0.796573752892 0.2\n",
      "After epoch:  5 0.2835 time:  15.1848568916\n",
      "0.00999999977648 0.583113958543 0.2\n",
      "0.00999999977648 0.605564913154 0.16\n",
      "0.00999999977648 0.455186375415 0.08\n",
      "0.00999999977648 0.682905862148 0.2\n",
      "0.00990197062492 0.550826931234 0.04\n",
      "0.00980488304049 0.673541680022 0.12\n",
      "0.00970968045294 0.614243516308 0.12\n",
      "0.00961630884558 0.636457668759 0.12\n",
      "0.00952471699566 0.568942973242 0.12\n",
      "0.00943485274911 0.648831177716 0.2\n",
      "0.00934666767716 0.634073186319 0.16\n",
      "0.00926011707634 0.548951893691 0.12\n",
      "0.00917515344918 0.604721965827 0.16\n",
      "0.00909173581749 0.593159965671 0.08\n",
      "0.00900982040912 1.04834778582 0.32\n",
      "0.00892936903983 0.725526561086 0.28\n",
      "After epoch:  6 0.2742 time:  15.1785309315\n",
      "0.00885034073144 0.506457861162 0.12\n",
      "0.00877269916236 0.353164295453 0.04\n",
      "0.00869640801102 0.666061186679 0.2\n",
      "0.00862143281847 0.790264791992 0.24\n",
      "0.00854773912579 0.58933913621 0.12\n",
      "0.00847529433668 0.592237140607 0.12\n",
      "0.00840406771749 0.760705098233 0.16\n",
      "0.00833402760327 0.794380426291 0.24\n",
      "0.00826514605433 0.54118808772 0.16\n",
      "0.00819739326835 0.852062928955 0.32\n",
      "0.00813074223697 0.745622216226 0.2\n",
      "0.00806516688317 0.771406847146 0.16\n",
      "0.00800064019859 0.568737499992 0.16\n",
      "0.00793713796884 0.458588446631 0.08\n",
      "0.00787463597953 0.605062725922 0.12\n",
      "0.00781311001629 0.767803503741 0.2\n",
      "After epoch:  7 0.2755 time:  15.1784009933\n",
      "0.00775253912434 0.39709945294 0.04\n",
      "0.00769289955497 0.357575273789 0.04\n",
      "0.00763417035341 0.58339420315 0.12\n",
      "0.00757633149624 0.314062241611 0.0\n",
      "0.00751936249435 0.397350470236 0.04\n",
      "0.00746324332431 0.449302731057 0.08\n",
      "0.00740795629099 0.50148825651 0.08\n",
      "0.00735348183662 0.743812966757 0.16\n",
      "0.00729980273172 0.42742675642 0.08\n",
      "0.00724690174684 0.871551778622 0.16\n",
      "0.00719476211816 0.470999817731 0.08\n",
      "0.00714336754754 0.509785982501 0.12\n",
      "0.00709270173684 0.635103510615 0.12\n",
      "0.00704274931923 0.551994948798 0.12\n",
      "0.00699349585921 0.501827802003 0.08\n",
      "0.00694492692128 0.622810725455 0.16\n",
      "After epoch:  8 0.2626 time:  15.1772229671\n",
      "0.00689702760428 0.297103500083 0.0\n",
      "0.00684978440404 0.406953250052 0.04\n",
      "0.00680318381637 0.310673193622 0.04\n",
      "0.00675721326843 0.464819196969 0.08\n",
      "0.00671185972169 0.422742736299 0.04\n",
      "0.00666711106896 0.67807364671 0.16\n",
      "0.00662295520306 0.36527371741 0.0\n",
      "0.00657938001677 0.401592242293 0.04\n",
      "0.00653637479991 0.643926266072 0.2\n",
      "0.00649392837659 0.508174132132 0.12\n",
      "0.00645202910528 0.348594613809 0.04\n",
      "0.00641066720709 0.650286048563 0.12\n",
      "0.00636983243749 0.43580873863 0.04\n",
      "0.00632951455191 0.423018574069 0.12\n",
      "0.00628970377147 0.658162254846 0.2\n",
      "0.00625039078295 0.374040672943 0.0\n",
      "After epoch:  9 0.2516 time:  15.1788630486\n",
      "0.00621156580746 0.337755613185 0.04\n",
      "0.0061732204631 0.492418170319 0.04\n",
      "0.00613534590229 0.34709976781 0.04\n",
      "0.00609793281183 0.408570874306 0.04\n",
      "0.00606097327545 0.327213614674 0.04\n",
      "0.00602445937693 0.306838194875 0.0\n",
      "0.00598838273436 0.470261877388 0.16\n",
      "0.00595273543149 0.603135040556 0.08\n",
      "0.00591751001775 0.291984515357 0.0\n",
      "0.00588269904256 0.482753680398 0.04\n",
      "0.00584829505533 0.361497111682 0.04\n",
      "0.00581429153681 0.350128161988 0.0\n",
      "0.00578068103641 0.415015787552 0.08\n",
      "0.00574745656922 0.459904472762 0.12\n",
      "0.00571461208165 0.656513128508 0.16\n",
      "0.00568214105442 0.329797895227 0.0\n",
      "After epoch:  10 0.2545 time:  15.1781520844\n",
      "0.00565003650263 0.265906465711 0.0\n",
      "0.005618293304 0.301148248397 0.0\n",
      "0.0055869044736 0.292755847661 0.0\n",
      "0.00555586442351 0.392372925281 0.04\n",
      "0.00552516710013 0.471942346421 0.08\n",
      "0.00549480738118 0.268783369701 0.0\n",
      "0.00546477967873 0.317628032065 0.04\n",
      "0.00543507793918 0.27770857985 0.0\n",
      "0.00540569750592 0.413253009155 0.08\n",
      "0.00537663325667 0.297594746776 0.04\n",
      "0.00534787960351 0.284556329785 0.0\n",
      "0.0053194318898 0.415791791852 0.04\n",
      "0.00529128545895 0.332743715183 0.04\n",
      "0.00526343472302 0.415631546754 0.08\n",
      "0.00523587642238 0.293295571822 0.0\n",
      "0.00520860450342 0.299225876597 0.0\n",
      "After epoch:  11 0.2447 time:  15.1803631783\n",
      "0.00518161570653 0.300074575187 0.04\n",
      "0.00515490490943 0.274361562874 0.0\n",
      "0.00512846792117 0.248038139669 0.0\n",
      "0.00510230101645 0.305105700508 0.04\n",
      "0.00507640000433 0.238950036913 0.0\n",
      "0.00505076022819 0.2619898354 0.0\n",
      "0.00502537796274 0.245881642222 0.0\n",
      "0.00500024994835 0.280872163465 0.0\n",
      "0.00497537199408 0.268011020735 0.0\n",
      "0.00495073990896 0.33211353652 0.04\n",
      "0.00492635089904 0.334012065726 0.04\n",
      "0.00490220123902 0.229237227013 0.0\n",
      "0.00487828673795 0.267242021526 0.0\n",
      "0.00485460460186 0.249684206168 0.0\n",
      "0.00483115110546 0.263420012622 0.0\n",
      "0.00480792345479 0.25579666094 0.0\n",
      "After epoch:  12 0.2458 time:  15.1794378757\n",
      "0.00478491792455 0.24342221954 0.0\n",
      "0.00476213172078 0.249787946995 0.0\n",
      "0.00473956111819 0.225667460038 0.0\n",
      "0.00471720378846 0.234546054101 0.0\n",
      "0.00469505600631 0.228277382282 0.0\n",
      "0.00467311544344 0.222801153915 0.0\n",
      "0.00465137930587 0.263482811096 0.0\n",
      "0.00462984386832 0.238969307793 0.0\n",
      "0.00460850726813 0.274133737769 0.0\n",
      "0.00458736624569 0.220496158756 0.0\n",
      "0.00456641847268 0.242151606227 0.04\n",
      "0.00454566115513 0.241036096559 0.0\n",
      "0.00452509149909 0.290273239264 0.04\n",
      "0.0045047076419 0.25335023658 0.0\n",
      "0.00448450585827 0.230463949032 0.0\n",
      "0.00446448521689 0.216138370758 0.0\n",
      "After epoch:  13 0.234 time:  15.1845710278\n",
      "0.00444464199245 0.210180232889 0.0\n",
      "0.00442497478798 0.212225952439 0.0\n",
      "0.00440548034385 0.201022363582 0.0\n",
      "0.00438615726307 0.218968324486 0.0\n",
      "0.00436700275168 0.221074812106 0.0\n",
      "0.00434801494703 0.203352587394 0.0\n",
      "0.00432919152081 0.203438183082 0.0\n",
      "0.00431053061038 0.207867338687 0.0\n",
      "0.00429202988744 0.196286705823 0.0\n",
      "0.00427368702367 0.193841976118 0.0\n",
      "0.00425550015643 0.191224319499 0.0\n",
      "0.00423746788874 0.213261576076 0.0\n",
      "0.00421958742663 0.194168035927 0.0\n",
      "0.00420185737312 0.1956691488 0.0\n",
      "0.00418427539989 0.190008726571 0.0\n",
      "0.00416684010997 0.19136746592 0.0\n",
      "After epoch:  14 0.2302 time:  15.1842250824\n",
      "0.00414954964072 0.189959946453 0.0\n",
      "0.00413240212947 0.185226624966 0.0\n",
      "0.0041153957136 0.1852056397 0.0\n",
      "0.00409852853045 0.180484063967 0.0\n",
      "0.00408179918304 0.188539648049 0.0\n",
      "0.00406520580873 0.19101894777 0.0\n",
      "0.00404874701053 0.176207118119 0.0\n",
      "0.00403242046013 0.177631787947 0.0\n",
      "0.00401622569188 0.17487662483 0.0\n",
      "0.00400015991181 0.178942623625 0.0\n",
      "0.00398422265425 0.17749811067 0.0\n",
      "0.0039684115909 0.174942806648 0.0\n",
      "0.00395272532478 0.177832933542 0.0\n",
      "0.00393716292456 0.169810036044 0.0\n",
      "0.00392172252759 0.170348977105 0.0\n",
      "0.0039064027369 0.170795099709 0.0\n",
      "After epoch:  15 0.2263 time:  15.1897878647\n",
      "0.00389120192267 0.168251985698 0.0\n",
      "0.00387611915357 0.165534035836 0.0\n",
      "0.00386115303263 0.169126630995 0.0\n",
      "0.00384630169719 0.162847386726 0.0\n",
      "0.00383156444877 0.161582337552 0.0\n",
      "0.00381693965755 0.175020825874 0.0\n",
      "0.00380242592655 0.160272597004 0.0\n",
      "0.00378802232444 0.16092194944 0.0\n",
      "0.00377372722141 0.163350755207 0.0\n",
      "0.00375953991897 0.156444072079 0.0\n",
      "0.00374545855448 0.159406042519 0.0\n",
      "0.00373148242943 0.157514607708 0.0\n",
      "0.00371761037968 0.157960795115 0.0\n",
      "0.00370384077542 0.161396375166 0.0\n",
      "0.00369017315097 0.15997011456 0.0\n",
      "0.0036766056437 0.16463780395 0.0\n",
      "After epoch:  16 0.2272 time:  15.1888370514\n",
      "0.00366313778795 0.15474576105 0.0\n",
      "0.00364976818673 0.154462140847 0.0\n",
      "0.00363649590872 0.154195797948 0.0\n",
      "0.00362331978977 0.151206770178 0.0\n",
      "0.00361023866571 0.154919443013 0.0\n",
      "0.00359725160524 0.151250088482 0.0\n",
      "0.00358435790986 0.151546288995 0.0\n",
      "0.00357155618258 0.152598341773 0.0\n",
      "0.00355884549208 0.147050975054 0.0\n",
      "0.00354622513987 0.143590149333 0.0\n",
      "0.00353369372897 0.153434948752 0.0\n",
      "0.00352125079371 0.147342882234 0.0\n",
      "0.00350889493711 0.146148938661 0.0\n",
      "0.00349662569351 0.141326878069 0.0\n",
      "0.00348444189876 0.152287990139 0.0\n",
      "0.00347234285437 0.141981645273 0.0\n",
      "After epoch:  17 0.2256 time:  15.1932032108\n",
      "0.00346032739617 0.140078702893 0.0\n",
      "0.00344839482568 0.141037789607 0.0\n",
      "0.00343654421158 0.140991581789 0.0\n",
      "0.00342477485538 0.139552556082 0.0\n",
      "0.00341308582574 0.144532815949 0.0\n",
      "0.00340147619136 0.139902742006 0.0\n",
      "0.00338994548656 0.134424812268 0.0\n",
      "0.0033784925472 0.141065992927 0.0\n",
      "0.00336711667478 0.137559935393 0.0\n",
      "0.00335581740364 0.137365953355 0.0\n",
      "0.00334459356964 0.143281271553 0.0\n",
      "0.00333344447426 0.13327743462 0.0\n",
      "0.00332236941904 0.132148319311 0.0\n",
      "0.0033113679383 0.137144746885 0.0\n",
      "0.00330043886788 0.135186132456 0.0\n",
      "0.00328958197497 0.129031510858 0.0\n",
      "After epoch:  18 0.2239 time:  15.1903870106\n",
      "0.0032787960954 0.131103478983 0.0\n",
      "0.00326808076352 0.130951144284 0.0\n",
      "0.003257435048 0.12835233086 0.0\n",
      "0.00324685871601 0.127662279539 0.0\n",
      "0.0032363506034 0.134231884677 0.0\n",
      "0.00322591047734 0.132779428708 0.0\n",
      "0.0032155374065 0.126090749141 0.0\n",
      "0.00320523092523 0.132677990153 0.0\n",
      "0.00319499033503 0.132918535246 0.0\n",
      "0.00318481470458 0.129793444256 0.0\n",
      "0.00317470403388 0.125618096574 0.0\n",
      "0.00316465715878 0.123471508071 0.0\n",
      "0.00315467361361 0.125830581182 0.0\n",
      "0.00314475293271 0.124433019157 0.0\n",
      "0.0031348944176 0.130752318276 0.0\n",
      "0.00312509760261 0.124866591614 0.0\n",
      "After epoch:  19 0.2252 time:  15.1944880486\n",
      "0.00311536178924 0.121443986052 0.0\n",
      "0.00310568651184 0.127453003177 0.0\n",
      "0.00309607107192 0.118144477444 0.0\n",
      "0.00308651500382 0.120888418178 0.0\n",
      "0.00307701784186 0.121652885211 0.0\n",
      "0.00306757865474 0.119047706962 0.0\n",
      "0.00305819744244 0.127132506751 0.0\n",
      "0.00304887350649 0.122185960786 0.0\n",
      "0.00303960614838 0.119335329911 0.0\n",
      "0.00303039490245 0.117939096712 0.0\n",
      "0.00302123930305 0.118648514701 0.0\n",
      "0.00301213888451 0.123779450154 0.0\n",
      "0.00300309318118 0.116031061682 0.0\n",
      "0.00299410172738 0.126329304011 0.0\n",
      "0.00298516382463 0.122586669531 0.0\n",
      "0.00297627900727 0.122091299009 0.0\n",
      "After epoch:  20 0.2233 time:  15.1924850941\n",
      "0.00296744704247 0.117200453366 0.0\n",
      "0.00295866746455 0.120393376106 0.0\n",
      "0.00294993957505 0.115629129652 0.0\n",
      "0.00294126290828 0.121725902375 0.0\n",
      "0.00293263723142 0.117254862264 0.0\n",
      "0.00292406207882 0.11229782945 0.0\n",
      "0.0029155369848 0.11343920037 0.0\n",
      "0.00290706125088 0.117668019988 0.0\n",
      "0.00289863464423 0.115137331923 0.0\n",
      "0.00289025693201 0.109618648148 0.0\n",
      "0.00288192741573 0.111007043964 0.0\n",
      "0.00287364586256 0.108689026326 0.0\n",
      "0.00286541157402 0.11391995018 0.0\n",
      "0.0028572245501 0.113884476082 0.0\n",
      "0.0028490840923 0.113671545127 0.0\n",
      "0.00284098973498 0.118694041619 0.0\n",
      "After epoch:  21 0.2241 time:  15.1962590218\n",
      "0.00283294147812 0.110999921496 0.0\n",
      "0.00282493862323 0.113367264077 0.0\n",
      "0.00281698070467 0.106506236401 0.0\n",
      "0.00280906772241 0.108695431144 0.0\n",
      "0.00280119897798 0.108968761257 0.0\n",
      "0.00279337400571 0.10588761455 0.0\n",
      "0.00278559280559 0.109895521099 0.0\n",
      "0.00277785491198 0.105234085388 0.0\n",
      "0.00277015985921 0.112157323004 0.0\n",
      "0.00276250718161 0.114512026119 0.0\n",
      "0.0027548968792 0.107043321245 0.0\n",
      "0.00274732825346 0.112213082856 0.0\n",
      "0.00273980107158 0.105881370414 0.0\n",
      "0.00273231510073 0.10632892685 0.0\n",
      "0.00272486987524 0.108929569467 0.0\n",
      "0.00271746516228 0.112577471099 0.0\n",
      "After epoch:  22 0.2237 time:  15.192898035\n",
      "0.00271010049619 0.105948139078 0.0\n",
      "0.00270277564414 0.105942959635 0.0\n",
      "0.0026954903733 0.10611521245 0.0\n",
      "0.00268824421801 0.110727634858 0.0\n",
      "0.00268103694543 0.106921545434 0.0\n",
      "0.00267386832274 0.107520805788 0.0\n",
      "0.00266673788428 0.104019130115 0.0\n",
      "0.00265964516439 0.10561707657 0.0\n",
      "0.00265259016305 0.106374848184 0.0\n",
      "0.00264557264745 0.102394181715 0.0\n",
      "0.00263859215192 0.104954379048 0.0\n",
      "0.00263164821081 0.103130591074 0.0\n",
      "0.0026247408241 0.103240237741 0.0\n",
      "0.00261786952615 0.103845217742 0.0\n",
      "0.00261103431694 0.104532054928 0.0\n",
      "0.00260423449799 0.110474835515 0.0\n",
      "After epoch:  23 0.2267 time:  15.1980490685\n",
      "0.0025974700693 0.102154127135 0.0\n",
      "0.00259074079804 0.113437064385 0.0\n",
      "0.00258404598571 0.102340204714 0.0\n",
      "0.00257738609798 0.0994585542495 0.0\n",
      "0.00257076020353 0.0984679170108 0.0\n",
      "0.00256416830234 0.100226971486 0.0\n",
      "0.0025576101616 0.098366333975 0.0\n",
      "0.00255108554848 0.101704128205 0.0\n",
      "0.0025445939973 0.0975653588132 0.0\n",
      "0.00253813550808 0.0981980453941 0.0\n",
      "0.00253170961514 0.098414979081 0.0\n",
      "0.0025253163185 0.0968751996926 0.0\n",
      "0.00251895515248 0.100833979389 0.0\n",
      "0.00251262588426 0.104575138708 0.0\n",
      "0.00250632851385 0.0937191651585 0.0\n",
      "0.00250006257556 0.0987306354805 0.0\n",
      "After epoch:  24 0.2251 time:  15.1934940815\n",
      "0.00249382783659 0.0982460126342 0.0\n",
      "0.00248762406409 0.0947512193966 0.0\n",
      "0.00248145125806 0.0966129168326 0.0\n",
      "0.00247530872002 0.101871273761 0.0\n",
      "0.0024691966828 0.09633168937 0.0\n",
      "0.00246311491355 0.102061579054 0.0\n",
      "0.0024570627138 0.100928269425 0.0\n",
      "0.0024510405492 0.0962891945731 0.0\n",
      "0.00244504748844 0.0949593444351 0.0\n",
      "0.00243908376433 0.0939790174494 0.0\n",
      "0.00243314914405 0.0967114130723 0.0\n",
      "0.00242724339478 0.101564253295 0.0\n",
      "0.00242136605084 0.102794282944 0.0\n",
      "0.00241551734507 0.0936959598799 0.0\n",
      "0.00240969657898 0.0978077364116 0.0\n",
      "0.0024039039854 0.092930412968 0.0\n",
      "After epoch:  25 0.2233 time:  15.1988880634\n",
      "0.00239813909866 0.092256395934 0.0\n",
      "0.00239240168594 0.0906348441883 0.0\n",
      "0.00238669174723 0.0947004136809 0.0\n",
      "0.00238100904971 0.0911322814706 0.0\n",
      "0.00237535336055 0.0893622636909 0.0\n",
      "0.00236972444691 0.0953742536577 0.0\n",
      "0.00236412207596 0.0905021767564 0.0\n",
      "0.00235854624771 0.0959644573037 0.0\n",
      "0.00235299649648 0.0990906196564 0.0\n",
      "0.00234747305512 0.0943801577902 0.0\n",
      "0.00234197522514 0.0897478395591 0.0\n",
      "0.00233650323935 0.0917717241363 0.0\n",
      "0.0023310566321 0.0974153911699 0.0\n",
      "0.00232563540339 0.0914455980797 0.0\n",
      "0.00232023955323 0.0947509608661 0.0\n",
      "0.00231486838311 0.0996110190562 0.0\n",
      "After epoch:  26 0.2237 time:  15.1968500614\n",
      "0.00230952212587 0.0914630315654 0.0\n",
      "0.00230420054868 0.092019698722 0.0\n",
      "0.0022989034187 0.0873452127923 0.0\n",
      "0.00229363050312 0.0898720765662 0.0\n",
      "0.00228838180192 0.0935235672062 0.0\n",
      "0.00228315708227 0.0912314675297 0.0\n",
      "0.00227795611136 0.0881479603516 0.0\n",
      "0.00227277888916 0.0884684278184 0.0\n",
      "0.00226762518287 0.0918775427147 0.0\n",
      "0.0022624945268 0.088296225205 0.0\n",
      "0.00225738738663 0.089313773946 0.0\n",
      "0.00225230306387 0.092199351841 0.0\n",
      "0.00224724155851 0.0881104784931 0.0\n",
      "0.00224220263772 0.0891423573773 0.0\n",
      "0.00223718653433 0.0939723804065 0.0\n",
      "0.00223219278269 0.0902962863048 0.0\n",
      "After epoch:  27 0.2252 time:  15.2005059719\n",
      "0.00222722114995 0.0854911787525 0.0\n",
      "0.00222227163613 0.0857879333674 0.0\n",
      "0.00221734400839 0.0967234438734 0.0\n",
      "0.00221243826672 0.0862218009885 0.0\n",
      "0.00220755417831 0.0879896421862 0.0\n",
      "0.00220269174315 0.0877203555235 0.0\n",
      "0.00219785049558 0.0832499139032 0.0\n",
      "0.00219303043559 0.0853781893845 0.0\n",
      "0.00218823179603 0.0901956189717 0.0\n",
      "0.00218345387839 0.0873028222481 0.0\n",
      "0.00217869668268 0.0876198820339 0.0\n",
      "0.00217396020889 0.0862876998212 0.0\n",
      "0.00216924445704 0.0843952587841 0.0\n",
      "0.00216454896145 0.0865616729214 0.0\n",
      "0.00215987395495 0.0917303627777 0.0\n",
      "0.00215521897189 0.0919152900888 0.0\n",
      "After epoch:  28 0.2243 time:  15.1976840496\n",
      "0.00215058377944 0.0854148953421 0.0\n",
      "0.00214596884325 0.0854514206484 0.0\n",
      "0.00214137346484 0.0852820962058 0.0\n",
      "0.00213679787703 0.0851294750995 0.0\n",
      "0.00213224161416 0.0830111777529 0.0\n",
      "0.00212770490907 0.084821408508 0.0\n",
      "0.00212318729609 0.0814505869053 0.0\n",
      "0.00211868900806 0.0873433780069 0.0\n",
      "0.0021142095793 0.0823374736339 0.0\n",
      "0.00210974924266 0.0808595512357 0.0\n",
      "0.00210530753247 0.0847108602761 0.0\n",
      "0.00210088444874 0.085864149412 0.0\n",
      "0.00209647999145 0.0816878297838 0.0\n",
      "0.00209209392779 0.0814408878996 0.0\n",
      "0.00208772625774 0.0844012673474 0.0\n",
      "0.00208337674849 0.080794534972 0.0\n",
      "After epoch:  29 0.2249 time:  15.2012310028\n",
      "0.00207904540002 0.083382091275 0.0\n",
      "0.00207473174669 0.0832154966603 0.0\n",
      "0.00207043625414 0.0823442582578 0.0\n",
      "0.00206615845673 0.0808401991592 0.0\n",
      "0.00206189812161 0.0811067441713 0.0\n",
      "0.00205765548162 0.0833996918566 0.0\n",
      "0.00205343030393 0.0787740519071 0.0\n",
      "0.00204922235571 0.0842745922962 0.0\n",
      "0.00204503163695 0.0825920834021 0.0\n",
      "0.00204085791484 0.0830222936979 0.0\n",
      "0.00203670142218 0.0827719250927 0.0\n",
      "0.00203256169334 0.087437945169 0.0\n",
      "0.0020284387283 0.087878332629 0.0\n",
      "0.00202433252707 0.0850085291379 0.0\n",
      "0.00202024285682 0.0773620338721 0.0\n",
      "0.00201616971754 0.086947051186 0.0\n",
      "After epoch:  30 0.2233 time:  15.1966519356\n",
      "0.0020121128764 0.0835784616446 0.0\n",
      "0.00200807256624 0.0796449463288 0.0\n",
      "0.00200404808857 0.0820601700543 0.0\n",
      "0.00200003990903 0.0792857286159 0.0\n",
      "0.00199604779482 0.0763439826042 0.0\n",
      "0.00199207151309 0.0764355484643 0.0\n",
      "0.00198811106384 0.0821181609583 0.0\n",
      "0.00198416644707 0.0798349117762 0.0\n",
      "0.00198023719713 0.0808178320274 0.0\n",
      "0.00197632354684 0.0821419380397 0.0\n",
      "0.00197242549621 0.0786896167053 0.0\n",
      "0.00196854257956 0.077584450978 0.0\n",
      "0.00196467502974 0.0826190920988 0.0\n",
      "0.00196082284674 0.0867834716611 0.0\n",
      "0.0019569855649 0.0792468562176 0.0\n",
      "0.00195316318423 0.0805163292446 0.0\n",
      "After epoch:  31 0.221 time:  15.2009279728\n",
      "0.00194935570471 0.0758556387266 0.0\n",
      "0.00194556312636 0.0830093528086 0.0\n",
      "0.00194178533275 0.0775511917933 0.0\n",
      "0.00193802209105 0.0760699834768 0.0\n",
      "0.00193427340128 0.0777624898355 0.0\n",
      "0.001930539147 0.0788726713739 0.0\n",
      "0.00192681944463 0.081051195474 0.0\n",
      "0.00192311394494 0.0761491100728 0.0\n",
      "0.00191942264792 0.075067695055 0.0\n",
      "0.00191574555356 0.0770286449637 0.0\n",
      "0.00191208242904 0.0770215565763 0.0\n",
      "0.00190843339078 0.0824049922916 0.0\n",
      "0.00190479820594 0.0770696148245 0.0\n",
      "0.00190117687453 0.0817230074641 0.0\n",
      "0.0018975691637 0.0777441115884 0.0\n",
      "0.0018939753063 0.0818784337938 0.0\n",
      "After epoch:  32 0.2241 time:  15.1979029179\n",
      "0.00189039495308 0.0726870326259 0.0\n",
      "0.00188682810403 0.0768828740893 0.0\n",
      "0.00188327464275 0.0753784454243 0.0\n",
      "0.00187973456923 0.0762562269846 0.0\n",
      "0.00187620776705 0.0784181569148 0.0\n",
      "0.00187269423623 0.0755054487623 0.0\n",
      "0.00186919386033 0.0764836131582 0.0\n",
      "0.00186570640653 0.0809685475283 0.0\n",
      "0.00186223210767 0.0751394080582 0.0\n",
      "0.00185877061449 0.0744230018096 0.0\n",
      "0.00185532204341 0.0746518621143 0.0\n",
      "0.0018518861616 0.0760982849936 0.0\n",
      "0.00184846296906 0.0746722942774 0.0\n",
      "0.0018450524658 0.0743225493469 0.0\n",
      "0.00184165453538 0.079355801157 0.0\n",
      "0.00183826906141 0.0796247496654 0.0\n",
      "After epoch:  33 0.2225 time:  15.2001419067\n",
      "0.00183489604387 0.0764453864201 0.0\n",
      "0.00183153536636 0.0729578494761 0.0\n",
      "0.00182818702888 0.0741460431548 0.0\n",
      "0.00182485079858 0.0772206535379 0.0\n",
      "0.0018215267919 0.0747100067181 0.0\n",
      "0.00181821489241 0.0723362241081 0.0\n",
      "0.0018149149837 0.0777639460807 0.0\n",
      "0.00181162706576 0.0756510901227 0.0\n",
      "0.00180835102219 0.0738414326488 0.0\n",
      "0.00180508673657 0.0735970109908 0.0\n",
      "0.00180183432531 0.0812274924243 0.0\n",
      "0.00179859355558 0.0795388713349 0.0\n",
      "0.00179536442738 0.0726636423597 0.0\n",
      "0.0017921468243 0.0732024090498 0.0\n",
      "0.00178894074634 0.079678719256 0.0\n",
      "0.00178574619349 0.0761322532029 0.0\n",
      "After epoch:  34 0.2242 time:  15.1974570751\n",
      "0.00178256293293 0.0725528585053 0.0\n",
      "0.00177939108107 0.0717602909307 0.0\n",
      "0.0017762305215 0.0769316826024 0.0\n",
      "0.0017730811378 0.0696195657204 0.0\n",
      "0.00176994281355 0.0765501245745 0.0\n",
      "0.00176681566518 0.0717303613298 0.0\n",
      "0.00176369957626 0.0708450176578 0.0\n",
      "0.00176059443038 0.0733787211602 0.0\n",
      "0.00175750011113 0.0757416400427 0.0\n",
      "0.00175441673491 0.0722320012331 0.0\n",
      "0.00175134418532 0.070423472327 0.0\n",
      "0.00174828234594 0.0723912969196 0.0\n",
      "0.00174523110036 0.0757554911253 0.0\n",
      "0.0017421906814 0.074208257666 0.0\n",
      "0.0017391606234 0.0718883434369 0.0\n",
      "0.00173614127561 0.0790800624314 0.0\n",
      "After epoch:  35 0.2241 time:  15.203138113\n",
      "0.00173313228879 0.06986023424 0.0\n",
      "0.00173013377935 0.0757993790574 0.0\n",
      "0.00172714551445 0.0695825499242 0.0\n",
      "0.00172416761052 0.0746130086211 0.0\n",
      "0.00172120006755 0.0708692695772 0.0\n",
      "0.0017182425363 0.0719141875359 0.0\n",
      "0.0017152952496 0.071192332796 0.0\n",
      "0.00171235809103 0.0711550046274 0.0\n",
      "0.00170943094417 0.0712071183605 0.0\n",
      "0.00170651380904 0.0743737650224 0.0\n",
      "0.00170360656921 0.0691383909908 0.0\n",
      "0.00170070922468 0.0703407739462 0.0\n",
      "0.00169782165904 0.0748967906033 0.0\n",
      "0.0016949439887 0.0705228495745 0.0\n",
      "0.00169207598083 0.0739775337789 0.0\n",
      "0.00168921775185 0.0741436411567 0.0\n",
      "After epoch:  36 0.2223 time:  15.1980860233\n",
      "0.00168636906892 0.073701221389 0.0\n",
      "0.00168353004847 0.0737807620145 0.0\n",
      "0.00168070057407 0.0710014591411 0.0\n",
      "0.00167788052931 0.0683047970063 0.0\n",
      "0.0016750699142 0.0732098043971 0.0\n",
      "0.00167226872873 0.0664675080327 0.0\n",
      "0.00166947697289 0.067841677819 0.0\n",
      "0.00166669441387 0.0685513429885 0.0\n",
      "0.00166392116807 0.0696780327666 0.0\n",
      "0.00166115711909 0.0719367278109 0.0\n",
      "0.00165840226691 0.070735172886 0.0\n",
      "0.00165565649513 0.0680043137438 0.0\n",
      "0.00165291992016 0.0692818513508 0.0\n",
      "0.00165019219276 0.0740040743322 0.0\n",
      "0.00164747354575 0.0666614775864 0.0\n",
      "0.00164476386271 0.0727451079107 0.0\n",
      "After epoch:  37 0.2213 time:  15.2027118206\n",
      "0.00164206314366 0.0667304692756 0.0\n",
      "0.00163937115576 0.0707858153144 0.0\n",
      "0.00163668801542 0.065758955695 0.0\n",
      "0.00163401360624 0.0707821458107 0.0\n",
      "0.0016313479282 0.0668321119135 0.0\n",
      "0.00162869098131 0.0679734386192 0.0\n",
      "0.00162604264915 0.06810413851 0.0\n",
      "0.00162340293173 0.0684793252375 0.0\n",
      "0.00162077182904 0.0668398160682 0.0\n",
      "0.00161814910825 0.0701585048239 0.0\n",
      "0.0016155350022 0.0684163444398 0.0\n",
      "0.00161292927805 0.0652858398403 0.0\n",
      "0.00161033193581 0.0702517517919 0.0\n",
      "0.00160774285905 0.0725593006773 0.0\n",
      "0.0016051621642 0.0664748925617 0.0\n",
      "0.00160258973483 0.0752717757833 0.0\n",
      "After epoch:  38 0.2244 time:  15.1995038986\n",
      "0.00160002557095 0.0668648609196 0.0\n",
      "0.00159746955615 0.0669740128301 0.0\n",
      "0.00159492180683 0.0642472348246 0.0\n",
      "0.00159238209017 0.0690334008375 0.0\n",
      "0.00158985040616 0.0671809243257 0.0\n",
      "0.00158732675482 0.0674563156652 0.0\n",
      "0.00158481113613 0.0704885193707 0.0\n",
      "0.00158230355009 0.0697911773276 0.0\n",
      "0.00157980376389 0.0662192781923 0.0\n",
      "0.00157731189393 0.0752306931099 0.0\n",
      "0.0015748279402 0.0758462579681 0.0\n",
      "0.00157235178631 0.0654562944425 0.0\n",
      "0.00156988331582 0.0681132148153 0.0\n",
      "0.00156742264517 0.0659076351882 0.0\n",
      "0.00156496977434 0.0662757728817 0.0\n",
      "0.0015625244705 0.06767185111 0.0\n",
      "After epoch:  39 0.2213 time:  15.2015488148\n",
      "0.00156008673366 0.0663598834183 0.0\n",
      "0.00155765668023 0.0678702068332 0.0\n",
      "0.0015552341938 0.0655360305448 0.0\n",
      "0.00155281915795 0.072025157498 0.0\n",
      "0.0015504116891 0.068057446483 0.0\n",
      "0.00154801155441 0.0671508394659 0.0\n",
      "0.00154561898671 0.0678931534403 0.0\n",
      "0.00154323363677 0.0748011824211 0.0\n",
      "0.0015408557374 0.0655330848379 0.0\n",
      "0.0015384851722 0.0638506433331 0.0\n",
      "0.00153612194117 0.0652589968368 0.0\n",
      "0.00153376581147 0.0680857929645 0.0\n",
      "0.00153141701594 0.0685605616709 0.0\n",
      "0.00152907532174 0.0664431649011 0.0\n",
      "0.00152674084529 0.0690164053279 0.0\n",
      "0.00152441347018 0.0689948523552 0.0\n",
      "After epoch:  40 0.2225 time:  15.2000410557\n",
      "0.0015220931964 0.0644322869115 0.0\n",
      "0.00151977990754 0.0665235982936 0.0\n",
      "0.00151747372001 0.0629430764463 0.0\n",
      "0.00151517451741 0.064236679273 0.0\n",
      "0.00151288218331 0.0682796185863 0.0\n",
      "0.00151059683412 0.0647311623937 0.0\n",
      "0.00150831835344 0.0662337581221 0.0\n",
      "0.00150604674127 0.0674112439099 0.0\n",
      "0.0015037819976 0.0676114255944 0.0\n",
      "0.00150152400602 0.0721259925574 0.0\n",
      "0.00149927288294 0.0693336562249 0.0\n",
      "0.00149702839553 0.0636600540712 0.0\n",
      "0.00149479066022 0.0659464843394 0.0\n",
      "0.00149255956057 0.0676947091571 0.0\n",
      "0.00149033521302 0.0639380844843 0.0\n",
      "0.00148811738472 0.0659958203585 0.0\n",
      "After epoch:  41 0.2216 time:  15.202037096\n",
      "0.0014859061921 0.0627932383786 0.0\n",
      "0.00148370151874 0.0645235097771 0.0\n",
      "0.00148150348105 0.0687268353153 0.0\n",
      "0.0014793118462 0.0662681663037 0.0\n",
      "0.00147712673061 0.0643279664764 0.0\n",
      "0.00147494801786 0.06730940421 0.0\n",
      "0.00147277570795 0.0654693169203 0.0\n",
      "0.00147060991731 0.0608787421564 0.0\n",
      "0.00146845029667 0.0641885221623 0.0\n",
      "0.00146629719529 0.0686345220872 0.0\n",
      "0.00146415026393 0.063086278733 0.0\n",
      "0.00146200973541 0.0661522155119 0.0\n",
      "0.0014598753769 0.0683680512069 0.0\n",
      "0.0014577471884 0.0610428055963 0.0\n",
      "0.00145562528633 0.0643143572144 0.0\n",
      "0.00145350955427 0.0672802091433 0.0\n",
      "After epoch:  42 0.2211 time:  15.2004380226\n",
      "0.0014513998758 0.0637162077312 0.0\n",
      "0.00144929636735 0.0604127698873 0.0\n",
      "0.0014471989125 0.0631794497413 0.0\n",
      "0.00144510762766 0.0663629890538 0.0\n",
      "0.00144302228 0.0608095297704 0.0\n",
      "0.00144094298594 0.0630419088116 0.0\n",
      "0.00143886962906 0.06407598635 0.0\n",
      "0.00143680220935 0.0636597363118 0.0\n",
      "0.00143474084325 0.0617230960122 0.0\n",
      "0.00143268529791 0.0698295180009 0.0\n",
      "0.00143063557334 0.0621465532809 0.0\n",
      "0.00142859178595 0.0649092043561 0.0\n",
      "0.00142655381933 0.0705464010539 0.0\n",
      "0.00142452167347 0.062994787628 0.0\n",
      "0.00142249534838 0.0633681027848 0.0\n",
      "0.00142047472764 0.0632784723344 0.0\n",
      "After epoch:  43 0.2247 time:  15.2032968998\n",
      "0.00141845981125 0.0613178519962 0.0\n",
      "0.00141645071562 0.0621058644627 0.0\n",
      "0.00141444720794 0.0638786541621 0.0\n",
      "0.00141244928818 0.0633056866808 0.0\n",
      "0.00141045707278 0.0619543198958 0.0\n",
      "0.00140847056173 0.0657492211379 0.0\n",
      "0.0014064895222 0.0638837917009 0.0\n",
      "0.0014045140706 0.0652544835946 0.0\n",
      "0.00140254420694 0.061102472926 0.0\n",
      "0.0014005798148 0.0681319163951 0.0\n",
      "0.00139862101059 0.0661422541816 0.0\n",
      "0.00139666756149 0.0662208875845 0.0\n",
      "0.00139471958391 0.0637537781882 0.0\n",
      "0.00139277707785 0.0646156058531 0.0\n",
      "0.0013908399269 0.0616772214228 0.0\n",
      "0.00138890813105 0.0629058077331 0.0\n",
      "After epoch:  44 0.2209 time:  15.2001361847\n",
      "0.00138698180672 0.0713090353355 0.0\n",
      "0.00138506072108 0.0614765969171 0.0\n",
      "0.00138314499054 0.0611820857476 0.0\n",
      "0.00138123449869 0.0614914453866 0.0\n",
      "0.00137932936195 0.0649722343032 0.0\n",
      "0.0013774294639 0.0631901804627 0.0\n",
      "0.00137553468812 0.0604883539478 0.0\n",
      "0.00137364526745 0.0616655271423 0.0\n",
      "0.00137176096905 0.0701000252505 0.0\n",
      "0.00136988179293 0.0595841554301 0.0\n",
      "0.00136800773907 0.0613329714935 0.0\n",
      "0.00136613892391 0.0653570024269 0.0\n",
      "0.00136427511461 0.059888570404 0.0\n",
      "0.00136241642758 0.0661986243356 0.0\n",
      "0.00136056274641 0.0616593815787 0.0\n",
      "0.0013587140711 0.0648081545529 0.0\n",
      "After epoch:  45 0.2222 time:  15.2025790215\n",
      "0.00135687051807 0.0608285304961 0.0\n",
      "0.00135503185447 0.0622707536101 0.0\n",
      "0.00135319831315 0.0632757667013 0.0\n",
      "0.00135136966128 0.0644019657145 0.0\n",
      "0.00134954589885 0.0642782003282 0.0\n",
      "0.00134772702586 0.0615389409912 0.0\n",
      "0.00134591315873 0.0621966288959 0.0\n",
      "0.00134410406463 0.0614649687475 0.0\n",
      "0.00134229985997 0.065414509524 0.0\n",
      "0.00134050054476 0.0599891969383 0.0\n",
      "0.00133870600257 0.0633416820311 0.0\n",
      "0.00133691623341 0.0612003726228 0.0\n",
      "0.0013351313537 0.0620850754871 0.0\n",
      "0.0013333511306 0.0642103483982 0.0\n",
      "0.00133157568052 0.0603781487417 0.0\n",
      "0.00132980488706 0.068563930442 0.0\n",
      "After epoch:  46 0.223 time:  15.2010149956\n",
      "0.00132803886663 0.0657317696647 0.0\n",
      "0.00132627750281 0.0601000816442 0.0\n",
      "0.00132452091202 0.0589539255626 0.0\n",
      "0.00132276886143 0.0615658604886 0.0\n",
      "0.00132102146745 0.0613441816577 0.0\n",
      "0.00131927861366 0.0619220512874 0.0\n",
      "0.00131754041649 0.0629005825666 0.0\n",
      "0.00131580675952 0.058133594356 0.0\n",
      "0.00131407775916 0.0651100391138 0.0\n",
      "0.00131235318258 0.0609064708966 0.0\n",
      "0.0013106331462 0.0592060723107 0.0\n",
      "0.00130891765002 0.0675183102853 0.0\n",
      "0.00130720657762 0.0576751042736 0.0\n",
      "0.00130550004542 0.0599422977706 0.0\n",
      "0.001303797937 0.0581413507224 0.0\n",
      "0.00130210025236 0.0617096429848 0.0\n",
      "After epoch:  47 0.222 time:  15.2038938999\n",
      "0.0013004069915 0.0566297337528 0.0\n",
      "0.00129871815443 0.0576432430632 0.0\n",
      "0.00129703374114 0.0583748155974 0.0\n",
      "0.0012953535188 0.0570966158192 0.0\n",
      "0.00129367783666 0.0552512937082 0.0\n",
      "0.00129200634547 0.0597459080872 0.0\n",
      "0.00129033927806 0.0657143628663 0.0\n",
      "0.0012886764016 0.0612621325349 0.0\n",
      "0.00128701783251 0.0587049572756 0.0\n",
      "0.00128536357079 0.0625021321234 0.0\n",
      "0.00128371350002 0.0604518586477 0.0\n",
      "0.00128206773661 0.0588686476412 0.0\n",
      "0.00128042616416 0.0593888652565 0.0\n",
      "0.00127878878266 0.0648116051582 0.0\n",
      "0.00127715547569 0.062638269745 0.0\n",
      "0.00127552647609 0.0641622209316 0.0\n",
      "After epoch:  48 0.2209 time:  15.2019219398\n",
      "0.00127390155103 0.0631481909751 0.0\n",
      "0.00127228081692 0.0591880431773 0.0\n",
      "0.00127066415735 0.0598658310426 0.0\n",
      "0.00126905168872 0.0576809508806 0.0\n",
      "0.00126744317822 0.0578794385851 0.0\n",
      "0.00126583885867 0.0576629250981 0.0\n",
      "0.00126423849724 0.062587701923 0.0\n",
      "0.00126264221035 0.0624005932617 0.0\n",
      "0.00126104999799 0.0578794976463 0.0\n",
      "0.00125946174376 0.0582053328816 0.0\n",
      "0.00125787744764 0.0619840032813 0.0\n",
      "0.00125629722606 0.0584847026312 0.0\n",
      "0.00125472084619 0.0582381128626 0.0\n",
      "0.00125314854085 0.0577092117498 0.0\n",
      "0.00125158007722 0.062520616962 0.0\n",
      "0.00125001557171 0.0654133763993 0.0\n",
      "After epoch:  49 0.2215 time:  15.2025411129\n",
      "0.00124845502432 0.0603724206388 0.0\n",
      "0.00124689831864 0.0643427293484 0.0\n",
      "0.00124534557108 0.0555195271467 0.0\n",
      "0.00124379654881 0.0567816905139 0.0\n",
      "0.00124225148465 0.0606239540531 0.0\n",
      "0.00124071014579 0.0579470002596 0.0\n",
      "0.00123917276505 0.065296063255 0.0\n",
      "0.0012376391096 0.0550681364277 0.0\n",
      "0.00123610917944 0.0595215788058 0.0\n",
      "0.00123458309099 0.0594009077914 0.0\n",
      "0.00123306084424 0.0660537258801 0.0\n",
      "0.00123154220637 0.0585013861082 0.0\n",
      "0.0012300274102 0.0619146145748 0.0\n",
      "0.00122851633932 0.0613212961187 0.0\n",
      "0.00122700887732 0.0597553604788 0.0\n",
      "0.00122550525703 0.060485475173 0.0\n",
      "After epoch:  50 0.2226 time:  15.2014908791\n",
      "0.0012240052456 0.0572730873025 0.0\n",
      "0.00122250884306 0.0567405336744 0.0\n",
      "0.0012210161658 0.0583120094705 0.0\n",
      "0.00121952709742 0.056570505376 0.0\n",
      "0.00121804163791 0.0590522411061 0.0\n",
      "0.00121655978728 0.0573728967269 0.0\n",
      "0.00121508154552 0.0549730854153 0.0\n",
      "0.00121360691264 0.0557467148712 0.0\n",
      "0.00121213588864 0.0563070966431 0.0\n",
      "0.00121066835709 0.0648502790915 0.0\n",
      "0.00120920443442 0.055919369369 0.0\n",
      "0.0012077440042 0.0570248538043 0.0\n",
      "0.00120628718287 0.0569915270266 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-d016dba30388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcifar10_train_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10000\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mpr\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlamb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258352/nn_assignments/libs/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_storage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequired\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m                 \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;31m# if we are allowing garbage collection, remove the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# conv improved smaler filter - one affine + bigger othera\n",
    "\n",
    "img_size = (32, 32)\n",
    "c1_i = 3\n",
    "c1_o = 50\n",
    "c1_f = 3\n",
    "p1   = 3\n",
    "p2   = 2\n",
    "c2_o = 50\n",
    "c2_f = 3\n",
    "hidden1 = 1000\n",
    "hidden2 = 500\n",
    "hidden3 = 900\n",
    "hidden4 = 2000\n",
    "hidden5 = 1500\n",
    "outs = 10\n",
    "gamma = 0.0005\n",
    "lamb = 0.9\n",
    "initC = 10.\n",
    "initW = 10.\n",
    "num_epochs  = 100\n",
    "\n",
    "net = FeedForwardNet([\n",
    "                      Conv(c1_o, c1_f),\n",
    "                      MaxPoolLayer(p1),\n",
    "                      Conv(c2_o, c2_f),\n",
    "                      MaxPoolLayer(p2),\n",
    "                      Flatten(\"Flatten\"),\n",
    "                      AffineLayer(hidden1, initW, gamma, \"tA\"),\n",
    "                      DropOutLayer(0.3),\n",
    "                      ReLULayer(\"ReLu\"),\n",
    "                      AffineLayer(outs, initW, gamma, \"tA\"), \n",
    "                      SoftMaxLayer(\"fSoftMax\")],lamb)\n",
    "net.build((c1_i, ) + img_size)\n",
    "print \"Start\"\n",
    "print \"gamma: \", gamma\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "#Noise\n",
    "u = Uniform(width=0.05)\n",
    "while e < num_epochs:\n",
    "    t0 = time.time()\n",
    "    for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "        alpha = 1e-2 * 10000 / np.maximum(10000, i)\n",
    "        pr ,c,a = net.trainFunction(X, Y.ravel(),alpha,lamb, gamma) #\n",
    "        i+=1\n",
    "        if i % 100 == 0:\n",
    "            print a, c, (pr  != Y.ravel()).mean()\n",
    "    t1 = time.time()\n",
    "    print \"After epoch: \", e, compute_er(net, cifar10_validation_stream), \"time: \", t1-t0\n",
    "    e+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2264"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_er(net, cifar10_test_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
